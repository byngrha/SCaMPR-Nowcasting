{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37f23ef-9499-4910-982e-6e0473723412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:30 [INFO] Searching for the latest 12 'GLB-5' files...\n",
      "2025-09-29 09:31:31 [INFO] Found 12 file links. Target met.\n",
      "2025-09-29 09:31:31 [INFO] Found 12 new files to download.\n",
      "2025-09-29 09:31:31 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290720000_e202509290729599_c202509290740324.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 3.74 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:34 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290730000_e202509290739599_c202509290756455.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 3.74 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:38 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290740000_e202509290749599_c202509290801041.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[==================================================] 3.75 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:42 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290750000_e202509290759599_c202509290811102.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[==================================================] 3.75 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:45 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290800000_e202509290809599_c202509290821500.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[==================================================] 3.75 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:48 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290810000_e202509290819599_c202509290831212.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 3.76 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:51 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290820000_e202509290829599_c202509290839205.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[==================================================] 3.77 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:55 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290830000_e202509290839599_c202509290851552.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 3.79 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:31:58 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290840000_e202509290849599_c202509290859309.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[==================================================] 3.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:32:01 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290850000_e202509290859599_c202509290909325.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 3.81 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:32:05 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290900000_e202509290909599_c202509290921524.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[==================================================] 3.82 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:32:08 [INFO] Downloading: RRQPE-INST-GLB-5_v1r1_blend_s202509290910000_e202509290919599_c202509290930032.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[==================================================] 3.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:32:11 [INFO] Cleaning directory '/cipslab_shared/cipslab/home/nugrahab/scampr/input/nc', keeping the newest 12 files.\n",
      "2025-09-29 09:32:11 [INFO] --- Process finished ---\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SCaMPR Data Downloader\n",
    "\n",
    "This script finds and downloads the latest 'GLB-5' data files from a public\n",
    "S3 bucket. It maintains a fixed number of the most recent files in a local\n",
    "directory, cleaning up older files as new ones are downloaded.\n",
    "\n",
    "The configuration is managed through a 'configSCaMPR.yaml' file.\n",
    "\"\"\"\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Third-Party Imports ---\n",
    "import boto3\n",
    "import requests\n",
    "import yaml\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Constants ---\n",
    "# Use the current working directory. This is safer for interactive environments like notebooks.\n",
    "SCRIPT_DIR = Path.cwd()\n",
    "CONFIG_FILE = SCRIPT_DIR / \"configSCaMPR.yaml\"\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configure basic logging for the script.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load and validate the YAML configuration file.\"\"\"\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        # Basic validation\n",
    "        required_keys = [\n",
    "            'bucket_name', 'root_prefix', 'search_hours_back',\n",
    "            'output_directory', 'max_files_to_keep'\n",
    "        ]\n",
    "        if not all(key in config for key in required_keys):\n",
    "            raise ValueError(\"Config file is missing one or more required keys.\")\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Config file not found at: {config_path}\")\n",
    "        sys.exit(1)\n",
    "    except (yaml.YAMLError, ValueError) as e:\n",
    "        logging.error(f\"Error processing config file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def find_latest_s3_links(config, num_files=12):\n",
    "    \"\"\"\n",
    "    Find the latest N file links from the S3 bucket that contain 'GLB-5'.\n",
    "\n",
    "    Args:\n",
    "        config (dict): The loaded configuration dictionary.\n",
    "        num_files (int): The number of latest file links to find.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of download URLs (oldest to newest).\n",
    "    \"\"\"\n",
    "    found_links = []\n",
    "    utc_now = datetime.utcnow()\n",
    "    s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "    logging.info(f\"Searching for the latest {num_files} 'GLB-5' files...\")\n",
    "\n",
    "    for i in range(config['search_hours_back']):\n",
    "        search_time = utc_now - timedelta(hours=i)\n",
    "        prefix = f\"{config['root_prefix']}/{search_time.strftime('%Y/%m/%d/%H')}/\"\n",
    "\n",
    "        try:\n",
    "            response = s3_client.list_objects_v2(\n",
    "                Bucket=config['bucket_name'], Prefix=prefix\n",
    "            )\n",
    "            if 'Contents' not in response:\n",
    "                continue\n",
    "\n",
    "            # Sort files in this prefix by key name, newest first\n",
    "            files_in_prefix = sorted(\n",
    "                [obj['Key'] for obj in response['Contents'] if not obj['Key'].endswith('/')],\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            for key in files_in_prefix:\n",
    "                if \"GLB-5\" in key:\n",
    "                    url = f\"https://{config['bucket_name']}.s3.amazonaws.com/{key}\"\n",
    "                    found_links.append(url)\n",
    "                    if len(found_links) >= num_files:\n",
    "                        logging.info(f\"Found {len(found_links)} file links. Target met.\")\n",
    "                        # Return sorted oldest to newest for sequential download\n",
    "                        return sorted(found_links)\n",
    "\n",
    "        except ClientError as e:\n",
    "            logging.warning(f\"S3 client error for prefix '{prefix}': {e}\")\n",
    "\n",
    "    logging.info(f\"Search complete. Found a total of {len(found_links)} 'GLB-5' files.\")\n",
    "    return sorted(found_links) # Return what was found, sorted oldest to newest\n",
    "\n",
    "def download_file(url, local_path, config):\n",
    "    \"\"\"\n",
    "    Download a single file from a URL with a progress bar.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the file to download.\n",
    "        local_path (Path): The local path to save the file to.\n",
    "        config (dict): The loaded configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if download was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Downloading: {local_path.name}\")\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=config.get('request_timeout_seconds', 30)) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            with open(local_path, 'wb') as f:\n",
    "                dl = 0\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    dl += len(chunk)\n",
    "                    f.write(chunk)\n",
    "                    if total_size > 0:\n",
    "                        done = int(50 * dl / total_size)\n",
    "                        sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl/1024/1024:.2f} MB\")\n",
    "                        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\n\") # Move to next line after progress bar\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Failed to download {url}: {e}\")\n",
    "        if local_path.exists():\n",
    "            os.remove(local_path) # Clean up partial download\n",
    "        return False\n",
    "\n",
    "def cleanup_old_files(directory, max_files):\n",
    "    \"\"\"\n",
    "    Ensure the directory contains at most max_files, deleting the oldest ones.\n",
    "\n",
    "    Args:\n",
    "        directory (Path): The directory to clean.\n",
    "        max_files (int): The maximum number of files to keep.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Cleaning directory '{directory}', keeping the newest {max_files} files.\")\n",
    "    try:\n",
    "        # Get all files and sort them by modification time (oldest first)\n",
    "        files = sorted(\n",
    "            [p for p in directory.iterdir() if p.is_file()],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        # Remove files until the count is at or below the max\n",
    "        while len(files) > max_files:\n",
    "            file_to_delete = files.pop(0)\n",
    "            logging.info(f\" -> Deleting old file: {file_to_delete.name}\")\n",
    "            os.remove(file_to_delete)\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"Cleanup directory '{directory}' not found. Skipping.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during file cleanup: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    setup_logging()\n",
    "    config = load_config(CONFIG_FILE)\n",
    "\n",
    "    output_dir = SCRIPT_DIR / config['output_directory']\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # 1. Find the latest file links from the S3 source\n",
    "    latest_links = find_latest_s3_links(config, num_files=12)\n",
    "    if not latest_links:\n",
    "        logging.info(\"No 'GLB-5' files found to download.\")\n",
    "        return\n",
    "\n",
    "    # 2. Determine which files are new and need to be downloaded\n",
    "    new_files_to_download = []\n",
    "    for link in latest_links:\n",
    "        local_path = output_dir / link.split('/')[-1]\n",
    "        if not local_path.exists():\n",
    "            new_files_to_download.append((link, local_path))\n",
    "\n",
    "    # 3. Download only the new files\n",
    "    if new_files_to_download:\n",
    "        logging.info(f\"Found {len(new_files_to_download)} new files to download.\")\n",
    "        for link, path in new_files_to_download:\n",
    "            download_file(link, path, config)\n",
    "    else:\n",
    "        logging.info(\"All latest files already exist locally. No downloads needed.\")\n",
    "\n",
    "    # 4. Clean up the output directory once after all operations\n",
    "    cleanup_old_files(output_dir, config['max_files_to_keep'])\n",
    "    logging.info(\"--- Process finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6ab93-b45d-4320-9878-dd4e64218dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pysteps]",
   "language": "python",
   "name": "conda-env-.conda-pysteps-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
